{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d54f6869",
   "metadata": {},
   "source": [
    "# ETL Pipeline Tutorial with DigitalHub\n",
    "\n",
    "This notebook demonstrates how to build an end-to-end ETL (Extract, Transform, Load) pipeline using the DigitalHub SDK. We'll work with Bologna's traffic data from open data APIs, process it through multiple stages, and deploy it as a REST API service.\n",
    "\n",
    "## Overview\n",
    "- **Extract**: Download traffic data from Bologna's open data portal\n",
    "- **Transform**: Process and clean the data in multiple stages\n",
    "- **Load**: Deploy the processed data as a queryable REST API\n",
    "- **Orchestrate**: Create a workflow pipeline to automate the entire process\n",
    "\n",
    "![pipeline](pipeline.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfa0dbb",
   "metadata": {},
   "source": [
    "## Project Initialization\n",
    "\n",
    "Initialize our DigitalHub project. This creates a space where we can manage our data items, functions, workflows, and deployments.\n",
    "\n",
    "We'll use the `username` as param to create a personal project in the shared space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6cab4-a33e-4682-8629-083e483b68fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "import getpass as gt\n",
    "\n",
    "USERNAME = gt.getuser()\n",
    "project = dh.get_or_create_project(f\"{USERNAME}-tutorial-project\")\n",
    "print(project.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9fa8f4-795c-4541-b9c8-31f6a0e95792",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "The first task is to explore the dataset. It is an Open Data published by Comune di Bologna with traffic measures collected in the city.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c094eb36-61d4-4496-a4a5-c230b4b1e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?limit=50000&lang=it&timezone=Europe%2FRome&use_labels=true&delimiter=%3B\"\n",
    "filename = \"rilevazione-flusso-veicoli-tramite-spire-anno-2023.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3451c1bf-1994-4ba8-9334-c3b359f36b84",
   "metadata": {},
   "source": [
    "Let's download the dataset and visualize it. We'll use `requests` library to fetch the remote artifact via HTTP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a3d03-4116-4dad-b003-538850f5fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "with requests.get(URL) as r:\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00db6bd-af0a-4885-9371-38774938014e",
   "metadata": {},
   "source": [
    "Open the file with the viewer on the left side: the CSV is separated by `;`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a8eb02-e411-4dfd-be4a-7c67d9412fc3",
   "metadata": {},
   "source": [
    "## Loading and normalizing the dataset\n",
    "\n",
    "We need to restructure the dataset, because the layout is not suitable for our intended usage. Let's load it with `pandas` and figure out how to transform it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50072a2f-9caa-49f5-a449-54c86b0bc41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(filename, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ffb6f-4ea6-447b-991c-746ac8c68277",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b99b16-e073-40f3-856b-6cb735d0b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8bf65-7688-4423-bac2-857ee608720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c66316-0e77-41ee-8a14-ac449b4d65b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c3b1a-92e7-4d3a-8818-a7d0db0a316d",
   "metadata": {},
   "source": [
    "The dataset contains ~313k rows with measures taken at different intervals during the day, for lots of different sensors (Spire). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909a9e9-5ef4-44a6-9b34-2606abbc65d4",
   "metadata": {},
   "source": [
    "## Loading with DigitalHub\n",
    "\n",
    "Instead of downloading the file ourselves, we can leverage the digitalhub sdk to read the remote dataset as **dataitem**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e705170d-a125-4c4b-af9e-222d5d27e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import digitalhub as dh\n",
    "\n",
    "di = project.new_dataitem(name=filename, kind=\"table\", path=URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e07e87-c0a3-42c1-a7b4-85a197b59280",
   "metadata": {},
   "outputs": [],
   "source": [
    "di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf7d0d3-53ea-41f4-a3fe-603d8e96ad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = di.as_df(sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db9f546-d8f3-4c85-b0b5-8a3863f17560",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740606dc-2e18-4351-80c3-328bed328fa9",
   "metadata": {},
   "source": [
    "## Data extraction\n",
    "\n",
    "As first step, let's extract the identifiers of the various sensors in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44c45fd-ea45-4fe0-9464-f5f76ded3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf= df.groupby(['codice spira']).first().reset_index()[['codice spira','longitudine','latitudine','Livello','tipologia','codice','codice arco','codice via','Nome via', 'stato','direzione','angolo','geopoint']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc81bcd-25b0-48e2-8155-e095b634367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3477483-10bf-4403-9a35-81ca51dbd2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4321287e-ef82-4035-852e-f6962fa399dc",
   "metadata": {},
   "source": [
    "There are >900 sensors in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d525120-b47d-4f35-ba07-8e21cc3ce11f",
   "metadata": {},
   "source": [
    "Let's reshape the dataset to extract a meaningful representation of the measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca43758-2246-410e-a30a-29976434ae12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KEYS = ['00:00-01:00', '01:00-02:00', '02:00-03:00', '03:00-04:00',\n",
    "        '04:00-05:00', '05:00-06:00', '06:00-07:00', '07:00-08:00',\n",
    "        '08:00-09:00', '09:00-10:00', '10:00-11:00', '11:00-12:00',\n",
    "        '12:00-13:00', '13:00-14:00', '14:00-15:00', '15:00-16:00',\n",
    "        '16:00-17:00', '17:00-18:00', '18:00-19:00', '19:00-20:00',\n",
    "        '20:00-21:00', '21:00-22:00', '22:00-23:00', '23:00-24:00']\n",
    "\n",
    "COLUMNS=['data','codice spira']\n",
    "\n",
    "rdf = df[COLUMNS+KEYS]\n",
    "ls = []\n",
    "for key in KEYS:\n",
    "    k = key.split(\"-\")[0]\n",
    "    xdf = rdf[COLUMNS + [key]]\n",
    "    xdf['time'] = xdf.data.apply(lambda x: x+' ' +k)\n",
    "    xdf['value'] = xdf[key]\n",
    "    ls.append(xdf[['time','codice spira','value']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80455cb3-2f93-46cf-bfd6-07adf9ce228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf= pd.concat(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011488c4-70a9-4dc4-a4ec-98e48601b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe87b5d-41e4-468a-945e-18690e93dabc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a78a911",
   "metadata": {},
   "source": [
    "## Setup and Function Definitions\n",
    "\n",
    "First, we'll create the necessary directory structure and define all the functions we'll need for our ETL pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa5b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"src\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f679df2",
   "metadata": {},
   "source": [
    "## Step 1: Data Extraction\n",
    "\n",
    "First step of our ETL pipeline - we'll create and run the `download-data` function to extract the raw CSV data from the Bologna open data portal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468c229",
   "metadata": {},
   "source": [
    "### Function Definitions\n",
    "\n",
    "This cell creates our functions file with the following components:\n",
    "\n",
    "- **`downloader`**: Extracts CSV data from the Bologna open data API\n",
    "\n",
    "Each function is decorated with `@handler` to integrate with the DigitalHub runtime system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaafa6a-e8c5-41a6-b5b1-822e03a6134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"src/downloader.py\"\n",
    "\n",
    "import pandas as pd\n",
    "from digitalhub_runtime_python import handler\n",
    "\n",
    "\n",
    "@handler(outputs=[\"dataset\"])\n",
    "def downloader(url):\n",
    "    return url.as_df(file_format='csv',sep=\";\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f93b4b-95f4-4843-b568-e160dd6b845c",
   "metadata": {},
   "source": [
    "Register the function (with source code) in the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc122e5-0823-4c02-af3d-5a021b35e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = project.new_function(\n",
    "    name=\"download-data\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    code_src=\"src/downloader.py\",\n",
    "    handler=\"downloader\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f0161-537f-4b92-bd13-3d6bfb0e8fae",
   "metadata": {},
   "source": [
    "Now we can run the function as batch job and verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2e5dbf-bfc1-49b6-8fc6-a00e88e78245",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = func.run(\"job\", inputs={\"url\": di.key}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b1e1bb",
   "metadata": {},
   "source": [
    "Let's examine the raw data we just downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df6a2bb-eadf-4397-85dd-fff1c2dd76d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_di = project.get_dataitem(\"dataset\")\n",
    "dataset_di.as_df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980a1c6f",
   "metadata": {},
   "source": [
    "## Step 2: Transform - Process Traffic Sensors (Spire)\n",
    "\n",
    "The first transformation extracts unique traffic sensor metadata. Since the raw data contains many duplicate sensor records, we'll group by sensor code and keep only the first occurrence to get clean sensor information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6010b74-19c5-4fb7-a17d-c2b416d9f759",
   "metadata": {},
   "source": [
    "### Function Definitions\n",
    "\n",
    "This cell creates our functions file with the following components:\n",
    "\n",
    "- **`process_spire`**: Transforms the raw data to extract unique traffic sensor information\n",
    "\n",
    "Each function is decorated with `@handler` to integrate with the DigitalHub runtime system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269de64d-ae74-4724-aab6-152cc80e92f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"src/process_spire.py\"\n",
    "\n",
    "import pandas as pd\n",
    "from digitalhub_runtime_python import handler\n",
    "\n",
    "\n",
    "COLS=['codice spira','longitudine','latitudine',\n",
    "      'Livello','tipologia','codice','codice arco',\n",
    "      'codice via','Nome via', 'stato','direzione',\n",
    "      'angolo','geopoint']\n",
    "\n",
    "\n",
    "@handler(outputs=[\"dataset-spire\"])\n",
    "def process_spire(di):\n",
    "    df = di.as_df()\n",
    "    return df.groupby(['codice spira']).first().reset_index()[COLS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5260e03b-13ee-42d5-8980-7d096f88d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_func = project.new_function(\n",
    "    name=\"process-spire\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    code_src=\"src/process_spire.py\",\n",
    "    handler=\"process_spire\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d4ed4-cc2e-4106-b7ef-0f9f382487f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_run = process_func.run(\"job\", inputs={\"di\": dataset_di.key}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9341db4",
   "metadata": {},
   "source": [
    "Let's see the cleaned sensor data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e57ec3-e276-4bee-a75a-539d4d09e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_run.output(\"dataset-spire\").as_df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9a26d",
   "metadata": {},
   "source": [
    "## Step 3: Transform - Process Traffic Measurements\n",
    "\n",
    "The second transformation reshapes the hourly traffic data from wide format (24 hour columns) to long format (time-value pairs). This makes the data more suitable for analysis and API queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6645f1-f323-4f42-841f-a1ed745f8905",
   "metadata": {},
   "source": [
    "### Function Definitions\n",
    "\n",
    "This cell creates our functions file with the following components:\n",
    "\n",
    "- **`process_measures`**: Reshapes time-series traffic measurements from wide to long format\n",
    "\n",
    "Each function is decorated with `@handler` to integrate with the DigitalHub runtime system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7e926-2acd-4c94-b4e2-9f7173f0210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"src/process_measures.py\"\n",
    "\n",
    "import pandas as pd\n",
    "from digitalhub_runtime_python import handler\n",
    "\n",
    "\n",
    "\n",
    "KEYS = ['00:00-01:00', '01:00-02:00', '02:00-03:00', '03:00-04:00',\n",
    "        '04:00-05:00', '05:00-06:00', '06:00-07:00', '07:00-08:00',\n",
    "        '08:00-09:00', '09:00-10:00', '10:00-11:00', '11:00-12:00',\n",
    "        '12:00-13:00', '13:00-14:00', '14:00-15:00', '15:00-16:00',\n",
    "        '16:00-17:00', '17:00-18:00', '18:00-19:00', '19:00-20:00',\n",
    "        '20:00-21:00', '21:00-22:00', '22:00-23:00', '23:00-24:00']\n",
    "COLUMNS=['data','codice spira']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@handler(outputs=[\"dataset-measures\"])\n",
    "def process_measures(di):\n",
    "    df = di.as_df()\n",
    "    rdf = df[COLUMNS+KEYS]\n",
    "    ls = []\n",
    "    for key in KEYS:\n",
    "        k = key.split(\"-\")[0]\n",
    "        xdf = rdf[COLUMNS + [key]]\n",
    "        xdf['time'] = xdf.data.apply(lambda x: x+' ' +k)\n",
    "        xdf['value'] = xdf[key]\n",
    "        ls.append(xdf[['time','codice spira','value']])\n",
    "    return pd.concat(ls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824c93b-e6f1-4316-b41b-bffe9a660c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_measures_func = project.new_function(\n",
    "    name=\"process-measures\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    code_src=\"src/process_measures.py\",\n",
    "    handler=\"process_measures\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f7218-eb0d-4976-a5c6-fa9183e49eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_measures_run = process_measures_func.run(\"job\", inputs={\"di\": dataset_di.key}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee04a77",
   "metadata": {},
   "source": [
    "Let's examine the transformed time-series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b39565c-d220-4214-a46e-84b9aa9f0c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "di_meas = process_measures_run.output(\"dataset-measures\")\n",
    "di_meas.as_df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fab1c78-4b32-44ec-93f2-1bb231e55c49",
   "metadata": {},
   "source": [
    "## Exercise: download and share the files\n",
    "\n",
    "Let's download a local copy of the dataset and then generate a downloadable pre-authorized link for sharing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d51cb1-01ee-419c-ae70-a6f28b100562",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = project.get_dataitem(\"dataset\").download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfcb2fa-f09b-48f4-8d9c-233ce6e98bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7586407c-c76d-48a1-8c77-c2c65844e2da",
   "metadata": {},
   "source": [
    "Maybe a CSV is more suitable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09afaeca-0347-4928-86b4-42c323dd424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"dataitem/dataset.csv\"\n",
    "project.get_dataitem(\"dataset\").as_df().to_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ff419-6677-4a58-96f9-c2b8df62f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e6bbea-641a-46d1-a0d4-d617ffe972be",
   "metadata": {},
   "source": [
    "Let's Generate the link for sharing directly from the datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b57d7-dbdc-4e78-af82-168199cac36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.get_dataitem(\"dataset\").spec.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac5343-9068-4cd4-b7f2-1f62f7b91d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def create_presigned_url(key,ext_url=None,expiration=3600):\n",
    "    try:\n",
    "        s3_client = boto3.client(\"s3\", endpoint_url=ext_url)        \n",
    "        u = urlparse(key)\n",
    "        response = s3_client.generate_presigned_url(\n",
    "            \"get_object\",\n",
    "            Params={\"Bucket\": u.netloc, \"Key\": str(u.path)[1:]},\n",
    "            ExpiresIn=expiration,\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise Exception(\"error reading from s3\")\n",
    "        return None\n",
    "\n",
    "    # The response contains the presigned URL\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac2a12-f3d5-4491-a489-9ec184b7e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_presigned_url(project.get_dataitem(\"dataset\").spec.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8821bf18-7ca5-491b-9077-b5e248dee4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04a21002",
   "metadata": {},
   "source": [
    "## Step 4: Load - Deploy as REST API\n",
    "\n",
    "Now we'll deploy our processed data as a REST API service. The API will support pagination and allow querying the traffic measurement data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa6b345-fc6e-4174-a1ca-b3a6f8a4e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"src/functions.py\"\n",
    "\n",
    "import pandas as pd\n",
    "from digitalhub_runtime_python import handler\n",
    "\n",
    "\n",
    "def init_context(context, dataitem):\n",
    "    di = context.project.get_dataitem(dataitem)\n",
    "    df = di.as_df()\n",
    "    setattr(context, \"df\", df)\n",
    "\n",
    "\n",
    "def serve(context, event):\n",
    "    df = context.df\n",
    "\n",
    "    if df is None:\n",
    "        return \"\"\n",
    "\n",
    "    # mock REST api\n",
    "    fields = event.fields\n",
    "\n",
    "    # pagination\n",
    "    page = 0\n",
    "    pageSize = 50\n",
    "\n",
    "    if \"page\" in fields:\n",
    "        page = int(fields[\"page\"])\n",
    "\n",
    "    if \"size\" in fields:\n",
    "        pageSize = int(fields[\"size\"])\n",
    "\n",
    "    if page < 0:\n",
    "        page = 0\n",
    "\n",
    "    if pageSize < 1:\n",
    "        pageSize = 1\n",
    "\n",
    "    if pageSize > 100:\n",
    "        pageSize = 100\n",
    "\n",
    "    start = page * pageSize\n",
    "    end = start + pageSize\n",
    "    total = len(df)\n",
    "\n",
    "    if end > total:\n",
    "        end = total\n",
    "\n",
    "    ds = df.iloc[start:end]\n",
    "    json = ds.to_json(orient=\"records\")\n",
    "\n",
    "    return {\"data\": json, \"page\": page, \"size\": pageSize, \"total\": total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e72d61-c896-4e7a-9ba4-f4b3a078f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_func = project.new_function(\n",
    "    name=\"api\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    code_src=\"src/functions.py\",\n",
    "    handler=\"serve\",\n",
    "    init_function=\"init_context\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041540f3-3a6e-4a99-8474-b0f9b4c1c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_serve_model = api_func.run(\"serve\", init_parameters={\"dataitem\": di_meas.key}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101db9f7",
   "metadata": {},
   "source": [
    "### Test the API\n",
    "\n",
    "Let's test our deployed API by making a request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796afbe4-1b14-45b2-8a87-cd89f4fb2d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "svc_url = f\"http://{run_serve_model.status.service['url']}/?page=5&size=10\"\n",
    "res = run_serve_model.invoke(url=svc_url).json()\n",
    "rdf = pd.read_json(res[\"data\"], orient=\"records\")\n",
    "rdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59accb31",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e38b8d5",
   "metadata": {},
   "source": [
    "Now let's create a workflow that orchestrates all these steps automatically. This pipeline uses Hera (Argo Workflows) to define the execution flow:\n",
    "\n",
    "1. **A**: Download data\n",
    "2. **B**: Process sensors (depends on A)\n",
    "3. **C**: Process measurements (depends on A) \n",
    "4. **D**: Deploy API (depends on C)\n",
    "\n",
    "The pipeline creates a DAG (Directed Acyclic Graph) where steps can run in parallel when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae47f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"src/pipeline.py\"\n",
    "from hera.workflows import Workflow, DAG, Parameter\n",
    "from digitalhub_runtime_hera.dsl import step\n",
    "\n",
    "\n",
    "def pipeline():\n",
    "    with Workflow(entrypoint=\"dag\", arguments=Parameter(name=\"url\")) as w:\n",
    "\n",
    "        with DAG(name=\"dag\"):\n",
    "            A = step(template={\"action\":\"job\", \"inputs\": {\"url\": \"{{workflow.parameters.url}}\"}},\n",
    "                     function=\"download-data\",\n",
    "                     outputs=[\"dataset\"])\n",
    "            B = step(template={\"action\":\"job\", \"inputs\": {\"di\": \"{{inputs.parameters.di}}\"}},\n",
    "                     function=\"process-spire\",\n",
    "                     inputs={\"di\": A.get_parameter(\"dataset\")})\n",
    "            C = step(template={\"action\":\"job\", \"inputs\": {\"di\": \"{{inputs.parameters.di}}\"}},\n",
    "                     function=\"process-measures\",\n",
    "                     inputs={\"di\": A.get_parameter(\"dataset\")},\n",
    "                     outputs=[\"dataset-measures\"])\n",
    "            D = step(template={\"action\": \"serve\", \"init_parameters\": {\"dataitem\": \"{{inputs.parameters.dataitem}}\"}},\n",
    "                     function=\"api\",\n",
    "                     inputs={\"dataitem\": C.get_parameter(\"dataset-measures\")})\n",
    "            A >> [B, C]\n",
    "            C >> D\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5f2d3f",
   "metadata": {},
   "source": [
    "### Execute the Complete Pipeline\n",
    "\n",
    "Finally, let's create and execute our complete ETL pipeline workflow. This will run all the steps we performed manually above, but in an automated, orchestrated manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ef4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = project.new_workflow(name=\"pipeline\", kind=\"hera\", code_src=\"src/pipeline.py\", handler=\"pipeline\")\n",
    "wf_run = workflow.run(\"build\", wait=True)\n",
    "wf_run = workflow.run(\"pipeline\", parameters={\"url\": di.key}, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ae1b3a-d2b9-4dae-923f-2cf5953858e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (OltreAI)",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
