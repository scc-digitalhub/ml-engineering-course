{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ae83e4-e820-4b2f-98c0-7c7f3dacb04f",
   "metadata": {},
   "source": [
    "# Agent webUI\n",
    "\n",
    "Let's build a web interface to test the agent. The interface will be available via browser by proxying the port through the workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e2dc5-ed41-4047-899b-e0829bd6423e",
   "metadata": {},
   "source": [
    "## Project Initialization\n",
    "\n",
    "Initialize a DigitalHub project using consistent naming with other tutorials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb4e74-b739-4d6d-8135-c26d3593bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "import getpass as gt\n",
    "\n",
    "USERNAME = gt.getuser()\n",
    "\n",
    "project = dh.get_or_create_project(f\"{USERNAME}-tutorial-project\")\n",
    "print(project.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a0bb5-2b88-4cfa-a40b-113dda171721",
   "metadata": {},
   "source": [
    "## 1. Fetch components \n",
    "\n",
    "Let's get the components definition. We are looking for the latest RUNNING run for the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa557f33-bfb6-4ea7-bad3-cdc32b54e26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_function = project.get_function(\"rag-service\")\n",
    "rag_run = rag_function.list_runs()[0]\n",
    "if rag_run:\n",
    "    print(f\"Found run {rag_run.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d5d20-b6cf-4108-a18b-b06ae481a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_function = project.get_function(\"embed\")\n",
    "embed_run = embed_function.list_runs()[0]\n",
    "if embed_run:\n",
    "    print(f\"Found run {embed_run.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9606355d-3f43-4a43-9c44-e834652508b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_function = project.get_function(\"llama32-1b\")\n",
    "llm_run = llm_function.list_runs()[0]\n",
    "if llm_run:\n",
    "    print(f\"Found run {llm_run.id}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb7e7d-0c69-417c-a780-ecc6161e71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_URL = embed_run.status.to_dict()[\"service\"][\"url\"]\n",
    "EMBED_MODEL = embed_run.status.to_dict()[\"openai\"][\"model\"]\n",
    "print(f\"service {EMBED_URL} with model {EMBED_MODEL}\")\n",
    "CHAT_URL = llm_run.status.to_dict()[\"service\"][\"url\"]\n",
    "CHAT_MODEL = llm_run.status.to_dict()[\"openai\"][\"model\"]\n",
    "print(f\"service {CHAT_URL} with model {CHAT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f7c6b3-248b-4072-a4de-9866803d059f",
   "metadata": {},
   "source": [
    "## Deploy the UI\n",
    "\n",
    "We use streamlit to serve a simple webpage with an input field connected to the agent API.\n",
    "\n",
    "[Streamlit](https://docs.streamlit.io/) is a Python framework to create browser applications with little code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ff244-9372-49cd-9846-1184f47d5661",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU streamlit dotenv transformers==4.50.3 psycopg[binary]==3.2.10 openai==1.109.1 langchain==0.3.27 langchain-core==0.3.76 langchain-openai==0.3.27 langchain-postgres==0.0.15 langchain-huggingface==0.3.1 langgraph==0.6.7 filelock==3.19.1 huggingface-hub==0.35.1 beautifulsoup4>=4.13.0 langchainhub==0.1.21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f255f863-ebe4-4602-ab14-3976c7eadcea",
   "metadata": {},
   "source": [
    "Add the models' names and service URLs to the environment file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f1e16-58b6-40b0-9ff0-32cbb75d5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./streamlit.env\", \"w\") as env_file:\n",
    "    env_file.write(f\"CHAT_MODEL_NAME={CHAT_MODEL}\\n\")\n",
    "    env_file.write(f\"CHAT_SERVICE_URL={CHAT_URL}\\n\")\n",
    "    env_file.write(f\"EMBEDDING_MODEL_NAME={EMBED_MODEL}\\n\")\n",
    "    env_file.write(f\"EMBEDDING_SERVICE_URL={EMBED_URL}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15bbcf8-c3db-40b4-8285-fec0e44af7ae",
   "metadata": {},
   "source": [
    "Write the function implementing the RAG ui to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd740ec6-4c90-4aac-9cfb-fd1e545b5ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile 'rag-streamlit-app.py'\n",
    "import os\n",
    "import bs4\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "from langgraph.graph import START, StateGraph\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Read environment variables\n",
    "add_env_path = Path('.') / 'streamlit.env'\n",
    "load_dotenv(dotenv_path=add_env_path, override=True)\n",
    "\n",
    "PG_USER = os.environ[\"DB_USERNAME\"]\n",
    "PG_PASS = os.environ[\"DB_PASSWORD\"]\n",
    "PG_HOST = os.environ[\"DB_HOST\"]\n",
    "PG_PORT = os.environ[\"DB_PORT\"]\n",
    "DB_NAME = os.environ[\"DB_DATABASE\"]\n",
    "ACCESS_TOKEN = os.environ[\"DHCORE_ACCESS_TOKEN\"]\n",
    "\n",
    "chat_model_name = os.environ[\"CHAT_MODEL_NAME\"]\n",
    "chat_service_url = os.environ[\"CHAT_SERVICE_URL\"]\n",
    "embedding_model_name = os.environ[\"EMBEDDING_MODEL_NAME\"]\n",
    "embedding_service_url = os.environ[\"EMBEDDING_SERVICE_URL\"]\n",
    "PG_CONN_URL = (\n",
    "    f\"postgresql+psycopg://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{DB_NAME}\"\n",
    ")\n",
    "\n",
    "# Embedding model\n",
    "class CEmbeddings(OpenAIEmbeddings):\n",
    "    def embed_documents(self, docs):\n",
    "        client = OpenAI(api_key=\"ignored\", base_url=f\"{embedding_service_url}/v1\")\n",
    "        emb_arr = []\n",
    "        for doc in docs:\n",
    "            #sanitize string: replace NUL with spaces\n",
    "            d=doc.replace(\"\\x00\", \"-\")            \n",
    "            embs = client.embeddings.create(\n",
    "                input=d,\n",
    "                model=embedding_model_name\n",
    "            )\n",
    "            emb_arr.append(embs.data[0].embedding)\n",
    "        return emb_arr\n",
    "\n",
    "custom_embeddings = CEmbeddings(api_key=\"ignored\")\n",
    "\n",
    "# Vector store\n",
    "vector_store = PGVector(\n",
    "    embeddings=custom_embeddings,\n",
    "    collection_name=f\"{embedding_model_name}_docs\",\n",
    "    connection=PG_CONN_URL,\n",
    ")\n",
    "\n",
    "# Chat model\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"ignore\"\n",
    "llm = init_chat_model(chat_model_name, model_provider=\"openai\", base_url=f\"{chat_service_url}/v1/\")\n",
    "\n",
    "# Define prompt and operations\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Define graph of operations\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Streamlit setup\n",
    "st.title(\"RAG App\")\n",
    "st.write(\"Welcome to the RAG (Retrieval-Augmented Generation) app.\")\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "qa = st.container()\n",
    "\n",
    "with st.form(\"rag_form\", clear_on_submit=True):\n",
    "    question = st.text_input(\"Question\", \"\")\n",
    "    submit = st.form_submit_button(\"Submit\")\n",
    "    \n",
    "if submit:\n",
    "    # Load and chunk contents\n",
    "    if question:\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
    "        with qa.chat_message(\"user\"):\n",
    "            st.write(question)\n",
    "    \n",
    "        response = graph.invoke({\"question\": question})\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response[\"answer\"]})\n",
    "        with qa.chat_message(\"assistant\"):\n",
    "            st.write(response[\"answer\"])\n",
    "    else:\n",
    "        with qa.chat_message(\"assistant\"):\n",
    "            st.write(\"You didn't provide a question!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2241d808-13ba-4236-a913-7221d399eefd",
   "metadata": {},
   "source": [
    "## Launch and test the Streamlit app\n",
    "This command launches the Streamlit app, based on the file written by the previous cell. To access the app, you will need to forward port 8501 in Coder. \n",
    "\n",
    "Try asking the app a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca42a65d-47a8-48ac-8121-b208cd9a79c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run rag-streamlit-app.py --browser.gatherUsageStats false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f2009-ea4f-4467-a164-c51a943d7fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c4c4601",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "\n",
    "* Modify the streaming app to consume the RAG service via HTTP\n",
    "* Package the application as container and run as container-serve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
